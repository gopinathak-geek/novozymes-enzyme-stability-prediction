{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopinathak-geek/novozymes-enzyme-stability-prediction/blob/main/NovoModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries "
      ],
      "metadata": {
        "id": "ap_NRiVAF5NU"
      },
      "id": "ap_NRiVAF5NU"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy import stats\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "4d77B_4K-WEa"
      },
      "id": "4d77B_4K-WEa",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the source data (CSV) files from Github"
      ],
      "metadata": {
        "id": "meW8JcrTHZJM"
      },
      "id": "meW8JcrTHZJM"
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = \"https://raw.githubusercontent.com/gopinathak-geek/novozymes-enzyme-stability-prediction/main/data/train.csv\"\n",
        "train_updates_csv = \"https://raw.githubusercontent.com/gopinathak-geek/novozymes-enzyme-stability-prediction/main/data/train_updates_20220929.csv\"\n",
        "test_csv = \"https://raw.githubusercontent.com/gopinathak-geek/novozymes-enzyme-stability-prediction/main/data/test.csv\""
      ],
      "metadata": {
        "id": "OsrA1NlwHXWv"
      },
      "id": "OsrA1NlwHXWv",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Loaded CSV files"
      ],
      "metadata": {
        "id": "7CuHBR0FH2ap"
      },
      "id": "7CuHBR0FH2ap"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(train_csv)\n",
        "train_updates_df = pd.read_csv(train_updates_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "test_seq_id = test_df['seq_id']"
      ],
      "metadata": {
        "id": "gYfw_qcJ-Vn6"
      },
      "id": "gYfw_qcJ-Vn6",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the train data with respect to the corrections in the train_updates data\n",
        "for more information: [see here](https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/356251)"
      ],
      "metadata": {
        "id": "4esHE0y6IH-L"
      },
      "id": "4esHE0y6IH-L"
    },
    {
      "cell_type": "code",
      "source": [
        "# seq_id of the row to be deleted in the train file\n",
        "seq_ids_to_delete = train_updates_df[train_updates_df[\"pH\"].isnull()][\"seq_id\"].values\n",
        "\n",
        "# seq_id of the row to be replaced in the train file\n",
        "seq_ids_to_replace = train_updates_df[train_updates_df[\"pH\"].notnull()][\"seq_id\"].values\n",
        "\n",
        "#drop the train rows\n",
        "train_df.drop(train_df[train_df.seq_id.isin(seq_ids_to_delete)].index, inplace=True)\n",
        "\n",
        "#replace the train rows with train update row\n",
        "train_df.loc[train_df.seq_id.isin(seq_ids_to_replace), [\"pH\", \"tm\"]] = train_updates_df[train_updates_df.seq_id.isin(seq_ids_to_replace)][[\"pH\", \"tm\"]].values"
      ],
      "metadata": {
        "id": "Ke7mXRdaBReJ"
      },
      "id": "Ke7mXRdaBReJ",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the data**"
      ],
      "metadata": {
        "id": "JsorKJ9YKChN"
      },
      "id": "JsorKJ9YKChN"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info(verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL7iCp9XJ6VR",
        "outputId": "eeb5855a-c5a1-46b6-b9a7-ac18ebe26c85"
      },
      "id": "lL7iCp9XJ6VR",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 28981 entries, 0 to 31389\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   seq_id            28981 non-null  int64  \n",
            " 1   protein_sequence  28981 non-null  object \n",
            " 2   pH                28695 non-null  float64\n",
            " 3   data_source       28001 non-null  object \n",
            " 4   tm                28981 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(2)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some of the PH values are null so delete those rows\n",
        "null_seq_ids = train_df[train_df[\"pH\"].isnull()][\"seq_id\"].values\n",
        "train_df.drop(train_df[train_df.seq_id.isin(null_seq_ids)].index, inplace=True)"
      ],
      "metadata": {
        "id": "U-32UrGrJ5_8"
      },
      "id": "U-32UrGrJ5_8",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info(verbose=True)"
      ],
      "metadata": {
        "id": "82mqBJxuHJZd",
        "outputId": "e4bb1eec-97b3-4639-a052-6878258b9df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "82mqBJxuHJZd",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 28695 entries, 0 to 31389\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   seq_id            28695 non-null  int64  \n",
            " 1   protein_sequence  28695 non-null  object \n",
            " 2   pH                28695 non-null  float64\n",
            " 3   data_source       27727 non-null  object \n",
            " 4   tm                28695 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(2)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()[['pH', 'tm']].transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "zwlzLcD-Luj0",
        "outputId": "7a088aac-8c74-4214-99e9-8d0b85013d9d"
      },
      "id": "zwlzLcD-Luj0",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      count       mean        std    min   25%   50%   75%    max\n",
              "pH  28695.0   6.872467   0.793184   1.99   7.0   7.0   7.0   11.0\n",
              "tm  28695.0  51.385604  12.076609  25.10  43.7  48.8  54.6  130.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ecaeb24f-b217-4b9e-bc9c-e7d75f26eb86\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pH</th>\n",
              "      <td>28695.0</td>\n",
              "      <td>6.872467</td>\n",
              "      <td>0.793184</td>\n",
              "      <td>1.99</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm</th>\n",
              "      <td>28695.0</td>\n",
              "      <td>51.385604</td>\n",
              "      <td>12.076609</td>\n",
              "      <td>25.10</td>\n",
              "      <td>43.7</td>\n",
              "      <td>48.8</td>\n",
              "      <td>54.6</td>\n",
              "      <td>130.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecaeb24f-b217-4b9e-bc9c-e7d75f26eb86')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ecaeb24f-b217-4b9e-bc9c-e7d75f26eb86 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ecaeb24f-b217-4b9e-bc9c-e7d75f26eb86');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique values of PH in train data \n",
        "train_df.pH.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CARd_66qOjtE",
        "outputId": "b035aa4e-a7d7-43cb-b2ca-756a77a00bad"
      },
      "id": "CARd_66qOjtE",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.  ,  7.5 ,  5.5 ,  6.2 ,  6.3 ,  8.5 ,  3.  ,  6.8 ,  6.4 ,\n",
              "        6.6 ,  6.7 ,  6.5 ,  4.5 ,  7.3 ,  5.  ,  6.  ,  4.  ,  3.5 ,\n",
              "        7.4 ,  4.2 ,  5.1 ,  7.1 ,  3.2 ,  2.5 ,  2.  ,  5.2 ,  7.2 ,\n",
              "        8.  ,  5.8 ,  7.8 ,  9.  ,  7.7 ,  7.6 , 10.  ,  8.2 ,  2.7 ,\n",
              "        2.8 ,  2.9 ,  3.1 ,  3.14,  2.53,  2.3 ,  2.2 ,  2.81,  8.25,\n",
              "        9.9 ,  9.5 , 10.4 ,  9.08,  3.7 ,  4.6 ,  4.4 ,  3.6 ,  5.4 ,\n",
              "        5.35,  5.38,  5.9 ,  5.28,  5.42,  6.1 ,  9.7 ,  8.6 ,  6.78,\n",
              "        5.3 ,  5.31,  3.02,  6.9 ,  5.7 ,  4.9 ,  4.3 ,  5.45,  3.01,\n",
              "        3.3 ,  3.8 ,  4.8 ,  2.4 ,  2.84,  2.34,  4.7 ,  2.1 ,  3.9 ,\n",
              "        2.02,  2.83,  2.03,  2.86,  1.99,  3.16,  2.94,  9.75,  3.15,\n",
              "        3.07,  3.04,  3.25,  3.21,  2.98,  3.19,  4.1 ,  5.6 ,  3.75,\n",
              "        4.75,  4.25,  9.6 , 11.  , 10.3 ])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows of ph less than 6 or greater than 8\n",
        "train_df.drop(train_df[train_df.pH > 8 ].index, inplace=True)\n",
        "train_df.drop(train_df[train_df.pH < 6 ].index, inplace=True)\n",
        "train_df.pH.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrzouK7nQMsc",
        "outputId": "2c174ecb-5d1d-486d-fc26-7ef33b4f65e2"
      },
      "id": "BrzouK7nQMsc",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.  , 7.5 , 6.2 , 6.3 , 6.8 , 6.4 , 6.6 , 6.7 , 6.5 , 7.3 , 6.  ,\n",
              "       7.4 , 7.1 , 7.2 , 8.  , 7.8 , 7.7 , 7.6 , 6.1 , 6.78, 6.9 ])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()[['pH', 'tm']].transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "dkpjXYYYSpmG",
        "outputId": "7fe89489-e2c5-4cd2-c008-eaf3bc43831b"
      },
      "id": "dkpjXYYYSpmG",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      count       mean        std   min   25%   50%   75%    max\n",
              "pH  26997.0   7.018893   0.161061   6.0   7.0   7.0   7.0    8.0\n",
              "tm  26997.0  51.239497  12.030520  25.1  43.6  48.6  54.2  130.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d08d6dc-c0d2-43e8-8a47-9bc0e99371c8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pH</th>\n",
              "      <td>26997.0</td>\n",
              "      <td>7.018893</td>\n",
              "      <td>0.161061</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tm</th>\n",
              "      <td>26997.0</td>\n",
              "      <td>51.239497</td>\n",
              "      <td>12.030520</td>\n",
              "      <td>25.1</td>\n",
              "      <td>43.6</td>\n",
              "      <td>48.6</td>\n",
              "      <td>54.2</td>\n",
              "      <td>130.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d08d6dc-c0d2-43e8-8a47-9bc0e99371c8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d08d6dc-c0d2-43e8-8a47-9bc0e99371c8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d08d6dc-c0d2-43e8-8a47-9bc0e99371c8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info(verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTrO0gKvSs6_",
        "outputId": "382dc0a1-17f7-4072-d3d2-9a7717b72fa2"
      },
      "id": "LTrO0gKvSs6_",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 26997 entries, 0 to 31389\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   seq_id            26997 non-null  int64  \n",
            " 1   protein_sequence  26997 non-null  object \n",
            " 2   pH                26997 non-null  float64\n",
            " 3   data_source       26521 non-null  object \n",
            " 4   tm                26997 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(2)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to return the molecular weight of the Amino acid"
      ],
      "metadata": {
        "id": "pkD5pZtUaAUa"
      },
      "id": "pkD5pZtUaAUa"
    },
    {
      "cell_type": "code",
      "source": [
        "def getMolecularWeight(aminoacid):\n",
        "  if aminoacid == \"A\": return 89\n",
        "  elif aminoacid == \"R\": return 174\n",
        "  elif aminoacid == \"N\": return 132\n",
        "  elif aminoacid == \"D\": return 133\n",
        "  elif aminoacid == \"B\": return 133\n",
        "  elif aminoacid == \"C\": return 121\n",
        "  elif aminoacid == \"Q\": return 146\n",
        "  elif aminoacid == \"E\": return 147\n",
        "  elif aminoacid == \"Z\": return 147\n",
        "  elif aminoacid == \"G\": return 75\n",
        "  elif aminoacid == \"H\": return 155\n",
        "  elif aminoacid == \"I\": return 131\n",
        "  elif aminoacid == \"L\": return 131\n",
        "  elif aminoacid == \"K\": return 146\n",
        "  elif aminoacid == \"M\": return 149\n",
        "  elif aminoacid == \"F\": return 165\n",
        "  elif aminoacid == \"P\": return 115\n",
        "  elif aminoacid == \"S\": return 105\n",
        "  elif aminoacid == \"T\": return 119\n",
        "  elif aminoacid == \"W\": return 204\n",
        "  elif aminoacid == \"Y\": return 181\n",
        "  elif aminoacid == \"V\": return 117\n",
        "  else: return 0"
      ],
      "metadata": {
        "id": "8FGtnDup0hih"
      },
      "id": "8FGtnDup0hih",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to return the tm and one channel image by replacing the protein sequence into sequence of molecular weight of the amino acid and PH"
      ],
      "metadata": {
        "id": "APHpR0XXaIBI"
      },
      "id": "APHpR0XXaIBI"
    },
    {
      "cell_type": "code",
      "source": [
        "def proteinSequenceToAmioAcidMolecularWeightWithPhAndLabel(data):\n",
        "  amionAcidSequence = []\n",
        "  img = []\n",
        "  label = []\n",
        "  for index, row in data.iterrows():\n",
        "    ph = row['pH']\n",
        "    tm = row['tm']\n",
        "    molecularWeightOfAminoAcidInSequence = []\n",
        "    pHofProteinSequence = []\n",
        "    for aminoacid in (row['protein_sequence']):\n",
        "      molecularWeight = getMolecularWeight(aminoacid)\n",
        "      molecularWeightOfAminoAcidInSequence.append(molecularWeight/110)\n",
        "      pHofProteinSequence.append(ph)\n",
        "    a = np.array([molecularWeightOfAminoAcidInSequence,pHofProteinSequence])\n",
        "    a = a[None, :]\n",
        "    img.append(a)\n",
        "    label.append(tm)\n",
        "  return [img, label]"
      ],
      "metadata": {
        "id": "fNcUi7YViuDN"
      },
      "id": "fNcUi7YViuDN",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to return the one channel image by replacing the protein sequence into sequence of molecular weight of the amino acid and PH"
      ],
      "metadata": {
        "id": "4Dr4UxWObBWE"
      },
      "id": "4Dr4UxWObBWE"
    },
    {
      "cell_type": "code",
      "source": [
        "def proteinSequenceToAmioAcidMolecularWeightWithPh(data):\n",
        "  amionAcidSequence = []\n",
        "  img = []\n",
        "  for index, row in data.iterrows():\n",
        "    ph = row['pH']\n",
        "    molecularWeightOfAminoAcidInSequence = []\n",
        "    pHofProteinSequence = []\n",
        "    for aminoacid in (row['protein_sequence']):\n",
        "      molecularWeight = getMolecularWeight(aminoacid)\n",
        "      molecularWeightOfAminoAcidInSequence.append(molecularWeight/110)\n",
        "      pHofProteinSequence.append(ph)\n",
        "    a = np.array([molecularWeightOfAminoAcidInSequence,pHofProteinSequence])\n",
        "    a = a[None, :]\n",
        "    img.append(a)\n",
        "  return img"
      ],
      "metadata": {
        "id": "qowPVV7HTbfV"
      },
      "id": "qowPVV7HTbfV",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img, train_label = proteinSequenceToAmioAcidMolecularWeightWithPhAndLabel(train_df)\n",
        "test_img = proteinSequenceToAmioAcidMolecularWeightWithPh(test_df)"
      ],
      "metadata": {
        "id": "kSiRJhI10q4K"
      },
      "id": "kSiRJhI10q4K",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to resize the protein sequence image into standard 2 X 220"
      ],
      "metadata": {
        "id": "4aPAltmNbJqs"
      },
      "id": "4aPAltmNbJqs"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "preprocess = T.Compose([\n",
        "   T.Resize((2,220)),\n",
        "])\n",
        "\n",
        "def change_shape(data):\n",
        "    torch_img = []\n",
        "    for im in (data):\n",
        "        im = torch.FloatTensor(im)\n",
        "        #transform = T.ToPILImage()\n",
        "        #img = transform(im)\n",
        "        x = preprocess(im)\n",
        "        x = x.cpu().detach().numpy()\n",
        "        torch_img.append(x)\n",
        "    return torch_img"
      ],
      "metadata": {
        "id": "qm9aJqhKqvcB"
      },
      "id": "qm9aJqhKqvcB",
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img = change_shape(train_img) # training data\n",
        "test_img = change_shape(test_img) # test data"
      ],
      "metadata": {
        "id": "Giam_-nbtRdI"
      },
      "id": "Giam_-nbtRdI",
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to convert into training dataset"
      ],
      "metadata": {
        "id": "lwNYJulfbqTr"
      },
      "id": "lwNYJulfbqTr"
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "metadata": {
        "id": "x3HsMHGetXdu"
      },
      "id": "x3HsMHGetXdu",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to convert into test dataset"
      ],
      "metadata": {
        "id": "kyZmPwhub4J-"
      },
      "id": "kyZmPwhub4J-"
    },
    {
      "cell_type": "code",
      "source": [
        "class TestData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "metadata": {
        "id": "WjYQtZXBVDqt"
      },
      "id": "WjYQtZXBVDqt",
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TrainData(torch.FloatTensor(train_img), torch.FloatTensor(train_label))\n",
        "test_data = TestData(torch.FloatTensor(test_img))"
      ],
      "metadata": {
        "id": "VJJiBNjStcik"
      },
      "id": "VJJiBNjStcik",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the training data into training data and validation data"
      ],
      "metadata": {
        "id": "I25MT9Szb-dk"
      },
      "id": "I25MT9Szb-dk"
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.6 * len(train_data))\n",
        "val_size = len(train_data) - train_size\n",
        "training_data, validation_data = torch.utils.data.random_split(train_data, [train_size, val_size])"
      ],
      "metadata": {
        "id": "mbJSo1MkCdLF"
      },
      "id": "mbJSo1MkCdLF",
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper parameter optimization**\n",
        "[Optuna - Hyper parameter optimization framework](https://optuna.org/)\n",
        "\n",
        "\n",
        "1.   Install Optuna framework\n",
        "\n"
      ],
      "metadata": {
        "id": "gVCacvO5cimr"
      },
      "id": "gVCacvO5cimr"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna"
      ],
      "metadata": {
        "id": "uk815zwvdset",
        "outputId": "1144cf4b-b610-42e5-f408-da046cb77bf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uk815zwvdset",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.8/dist-packages (3.0.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.8.1)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.8/dist-packages (from optuna) (4.1.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.44)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from optuna) (0.9.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<5.0.0->optuna) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (4.1.1)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (2.4.2)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (0.5.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from stevedore>=2.0.1->cliff->optuna) (5.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2.   Import the required libraries\n"
      ],
      "metadata": {
        "id": "cGby_ZTedZlc"
      },
      "id": "cGby_ZTedZlc"
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.trial import TrialState"
      ],
      "metadata": {
        "id": "-He4mGaRdsZ2"
      },
      "id": "-He4mGaRdsZ2",
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3.   Define the Device, batch size, number of units in output layer, Epochs, Criterion.\n",
        "\n"
      ],
      "metadata": {
        "id": "7DQwUGwmd1hJ"
      },
      "id": "7DQwUGwmd1hJ"
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCHSIZE = 128\n",
        "OUTPUT = 1\n",
        "EPOCHS = 20\n",
        "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
        "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "iPz6ab4AdsTW"
      },
      "id": "iPz6ab4AdsTW",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Trial model for hyper parameter optimization and the parameters are\n",
        "\n",
        "\n",
        "1.   Number of Fully connected layers in the CNN\n",
        "2.   Number of units in each layer\n",
        "3.   Drop out\n",
        "4.   optimizer\n",
        "5.   Learning rate\n",
        "\n"
      ],
      "metadata": {
        "id": "EW-9yl8zeSql"
      },
      "id": "EW-9yl8zeSql"
    },
    {
      "cell_type": "code",
      "source": [
        "class NovoNetTrial(nn.Module):\n",
        "    def __init__(self, trial, name='NovoNetTrial'):\n",
        "        super(NovoNetTrial, self).__init__()\n",
        "        if name:\n",
        "            self.name = name\n",
        "        self.conv1 = nn.Conv2d(1, 2, (2,3), stride=(2, 1), padding=(1, 1))\n",
        "        self.pool = nn.MaxPool2d((2, 3), stride=(2, 2), padding=(1,0))\n",
        "        self.conv2 = nn.Conv2d(2, 6, (2,3), stride=(2, 1), padding=(1, 1))\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        in_features = 6*2*54\n",
        "        for i in range(n_layers):\n",
        "          out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
        "          self.layers.append(nn.Linear(in_features, out_features))\n",
        "          self.layers.append(nn.ReLU())\n",
        "          p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
        "          self.layers.append(nn.Dropout(p))\n",
        "\n",
        "          in_features = out_features\n",
        "        self.layers.append(nn.Linear(in_features, OUTPUT))\n",
        "\n",
        "        # compute the total number of parameters\n",
        "        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(self.name + ': total params:', total_params)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 6 * 2 * 54)\n",
        "\n",
        "        for i, conv_i in enumerate(self.layers): \n",
        "          x = conv_i(x) \n",
        "        return x"
      ],
      "metadata": {
        "id": "JltXxnqWdsMr"
      },
      "id": "JltXxnqWdsMr",
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a objective function for hyperparameter optimization whch retuns the accuracy"
      ],
      "metadata": {
        "id": "A9ar6qvQfruo"
      },
      "id": "A9ar6qvQfruo"
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "    # Generate the model.\n",
        "    model = NovoNetTrial(trial).to(DEVICE)\n",
        "\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "    train_loader = DataLoader(dataset=training_data, batch_size=BATCHSIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    valid_loader = DataLoader(dataset=validation_data, batch_size=BATCHSIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for batch_idx, (img, label) in enumerate(train_loader):\n",
        "            # Limiting training data for faster epochs.\n",
        "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
        "                break\n",
        "\n",
        "            img, label = img.to(DEVICE), label.to(DEVICE)\n",
        "            label = label.unsqueeze(1)\n",
        "           \n",
        "            optimizer.zero_grad()\n",
        "            output = model(img)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation of the model.\n",
        "        model.eval()\n",
        "        trial_validation_score = []\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (img, label) in enumerate(valid_loader):\n",
        "                # Limiting validation data.\n",
        "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                    break\n",
        "\n",
        "                img, label = img.to(DEVICE), label.to(DEVICE)\n",
        "                label = label.unsqueeze(1)\n",
        "                output = model(img)\n",
        "                \n",
        "                trial_val_result = stats.spearmanr(output.detach().cpu().numpy(), label.cpu().numpy())\n",
        "                trial_validation_score.append(trial_val_result.correlation)\n",
        "        \n",
        "                \n",
        "        accuracy = np.mean(trial_validation_score)\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "U2dDJBUwdsFQ"
      },
      "id": "U2dDJBUwdsFQ",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Study the optuna with 100 trials which gives the mazimum value"
      ],
      "metadata": {
        "id": "r5udkJ14hEv4"
      },
      "id": "r5udkJ14hEv4"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=100, timeout=600)\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "As-u9yp-dr9K",
        "outputId": "c605ee5f-363b-489a-d5b8-078083ba5b6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "As-u9yp-dr9K",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:03:11,348]\u001b[0m A new study created in memory with name: no-name-90baf627-d59c-4820-a9bb-f91d19eea83e\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 73543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:03:27,493]\u001b[0m Trial 0 finished with value: 0.10013303224560184 and parameters: {'n_layers': 1, 'n_units_l0': 113, 'dropout_l0': 0.28445483445006653, 'optimizer': 'RMSprop', 'lr': 2.7904834417604343e-05}. Best is trial 0 with value: 0.10013303224560184.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 49510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:03:42,167]\u001b[0m Trial 1 finished with value: 0.17754734346096207 and parameters: {'n_layers': 3, 'n_units_l0': 53, 'dropout_l0': 0.22491663716208532, 'n_units_l1': 120, 'dropout_l1': 0.32966886065688356, 'n_units_l2': 70, 'dropout_l2': 0.40450282919630687, 'optimizer': 'RMSprop', 'lr': 0.002543530633186278}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 46891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:03:55,533]\u001b[0m Trial 2 finished with value: 0.009625409604578774 and parameters: {'n_layers': 2, 'n_units_l0': 70, 'dropout_l0': 0.41268217450461314, 'n_units_l1': 19, 'dropout_l1': 0.2725618747912793, 'optimizer': 'RMSprop', 'lr': 0.0006785590953821994}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 37143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:07,670]\u001b[0m Trial 3 finished with value: 0.05707433578322037 and parameters: {'n_layers': 1, 'n_units_l0': 57, 'dropout_l0': 0.4683513328078628, 'optimizer': 'Adam', 'lr': 0.03719820886655435}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 61526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:20,140]\u001b[0m Trial 4 finished with value: 0.023627336527913746 and parameters: {'n_layers': 3, 'n_units_l0': 85, 'dropout_l0': 0.32895900235090747, 'n_units_l1': 33, 'dropout_l1': 0.41434540208131926, 'n_units_l2': 98, 'dropout_l2': 0.3761734447288724, 'optimizer': 'RMSprop', 'lr': 0.0009480923278046947}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 52278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/optuna/pruners/_percentile.py:20: RuntimeWarning: All-NaN slice encountered\n",
            "  return np.nanmax(values)\n",
            "\u001b[32m[I 2022-12-04 08:04:21,010]\u001b[0m Trial 5 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 31572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:21,910]\u001b[0m Trial 6 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 31943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:22,540]\u001b[0m Trial 7 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 44293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/scipy/stats/stats.py:4484: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(SpearmanRConstantInputWarning())\n",
            "\u001b[32m[I 2022-12-04 08:04:23,176]\u001b[0m Trial 8 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 13273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:23,810]\u001b[0m Trial 9 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 7834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:27,415]\u001b[0m Trial 10 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 88465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:31,660]\u001b[0m Trial 11 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 97128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:34,166]\u001b[0m Trial 12 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 63793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:37,188]\u001b[0m Trial 13 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 24125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:41,462]\u001b[0m Trial 14 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 63143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:42,113]\u001b[0m Trial 15 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 77232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:42,736]\u001b[0m Trial 16 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 23365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:43,410]\u001b[0m Trial 17 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 84751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:47,720]\u001b[0m Trial 18 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 55993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:48,342]\u001b[0m Trial 19 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 83562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:48,997]\u001b[0m Trial 20 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 35193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:49,631]\u001b[0m Trial 21 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 37793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:53,961]\u001b[0m Trial 22 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 24143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:54,605]\u001b[0m Trial 23 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 38443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:04:55,266]\u001b[0m Trial 24 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 50793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:04:59,586]\u001b[0m Trial 25 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 31905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:00,250]\u001b[0m Trial 26 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 16343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:03,331]\u001b[0m Trial 27 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 48451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:05,935]\u001b[0m Trial 28 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 57729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:06,554]\u001b[0m Trial 29 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 32593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:09,573]\u001b[0m Trial 30 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 66044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:22,054]\u001b[0m Trial 31 finished with value: 0.1436575085518907 and parameters: {'n_layers': 3, 'n_units_l0': 89, 'dropout_l0': 0.35043495591619545, 'n_units_l1': 43, 'dropout_l1': 0.39652960456792863, 'n_units_l2': 96, 'dropout_l2': 0.3763574154420193, 'optimizer': 'RMSprop', 'lr': 0.0005499942230941981}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 65688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:24,616]\u001b[0m Trial 32 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 76931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:27,152]\u001b[0m Trial 33 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 73343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:27,806]\u001b[0m Trial 34 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 46921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:30,920]\u001b[0m Trial 35 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 49781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:32,174]\u001b[0m Trial 36 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 49982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:33,499]\u001b[0m Trial 37 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 93476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:34,152]\u001b[0m Trial 38 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 59595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:34,809]\u001b[0m Trial 39 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 33893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:35,496]\u001b[0m Trial 40 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 61350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:36,164]\u001b[0m Trial 41 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 36856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:36,825]\u001b[0m Trial 42 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 74242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:37,489]\u001b[0m Trial 43 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 72206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:38,163]\u001b[0m Trial 44 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 48435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:38,811]\u001b[0m Trial 45 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 79640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:39,493]\u001b[0m Trial 46 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 59910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:40,146]\u001b[0m Trial 47 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 65755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:52,613]\u001b[0m Trial 48 finished with value: 0.13749763007916999 and parameters: {'n_layers': 2, 'n_units_l0': 91, 'dropout_l0': 0.2884938126319594, 'n_units_l1': 71, 'dropout_l1': 0.4315636183784945, 'optimizer': 'RMSprop', 'lr': 0.002471371224052419}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 66663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:53,265]\u001b[0m Trial 49 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 82643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:05:55,846]\u001b[0m Trial 50 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 67441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:56,505]\u001b[0m Trial 51 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 52743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:57,168]\u001b[0m Trial 52 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 70402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:05:59,701]\u001b[0m Trial 53 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 41919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:01,640]\u001b[0m Trial 54 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 43643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:14,042]\u001b[0m Trial 55 finished with value: 0.07460625364808007 and parameters: {'n_layers': 1, 'n_units_l0': 67, 'dropout_l0': 0.2531176027870232, 'optimizer': 'RMSprop', 'lr': 9.82389098773601e-05}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 35843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:14,704]\u001b[0m Trial 56 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 42993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:15,367]\u001b[0m Trial 57 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 28043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:16,025]\u001b[0m Trial 58 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 47543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:16,658]\u001b[0m Trial 59 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 16993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:17,326]\u001b[0m Trial 60 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 74053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:17,981]\u001b[0m Trial 61 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 56643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:18,902]\u001b[0m Trial 62 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 65743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:19,894]\u001b[0m Trial 63 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 10254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:20,908]\u001b[0m Trial 64 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 31293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:21,589]\u001b[0m Trial 65 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 46473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:23,462]\u001b[0m Trial 66 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 58596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:24,164]\u001b[0m Trial 67 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 43643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:27,331]\u001b[0m Trial 68 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 67136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:28,639]\u001b[0m Trial 69 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 79393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:41,056]\u001b[0m Trial 70 finished with value: 0.15506372705074653 and parameters: {'n_layers': 1, 'n_units_l0': 122, 'dropout_l0': 0.22838751463584964, 'optimizer': 'RMSprop', 'lr': 0.0010853324074018129}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 79393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:41,704]\u001b[0m Trial 71 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 76143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:42,365]\u001b[0m Trial 72 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 24793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:43,014]\u001b[0m Trial 73 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 83293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:43,681]\u001b[0m Trial 74 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 72243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:44,324]\u001b[0m Trial 75 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 50636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:45,652]\u001b[0m Trial 76 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 80693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:46,326]\u001b[0m Trial 77 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 103957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:47,022]\u001b[0m Trial 78 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 81967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:47,691]\u001b[0m Trial 79 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 71759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:48,390]\u001b[0m Trial 80 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 43279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:49,085]\u001b[0m Trial 81 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 52994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:49,756]\u001b[0m Trial 82 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 71665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:50,433]\u001b[0m Trial 83 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 49107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:51,103]\u001b[0m Trial 84 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 44943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:51,759]\u001b[0m Trial 85 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 41611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:52,437]\u001b[0m Trial 86 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 37245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:53,097]\u001b[0m Trial 87 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 41388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:53,785]\u001b[0m Trial 88 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 52093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:55,083]\u001b[0m Trial 89 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 68715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:06:55,783]\u001b[0m Trial 90 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 21859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:06:56,467]\u001b[0m Trial 91 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 11035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[32m[I 2022-12-04 08:07:09,002]\u001b[0m Trial 92 finished with value: 0.16740944514948236 and parameters: {'n_layers': 2, 'n_units_l0': 14, 'dropout_l0': 0.23023763719540646, 'n_units_l1': 116, 'dropout_l1': 0.4852093393498506, 'optimizer': 'RMSprop', 'lr': 0.0036288423648184846}. Best is trial 1 with value: 0.17754734346096207.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 14881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:07:09,668]\u001b[0m Trial 93 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 10843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:07:10,330]\u001b[0m Trial 94 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 57623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:07:11,007]\u001b[0m Trial 95 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 54693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:07:11,663]\u001b[0m Trial 96 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 90329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:07:12,379]\u001b[0m Trial 97 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 61843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:07:13,032]\u001b[0m Trial 98 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NovoNetTrial: total params: 3337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-04 08:07:13,675]\u001b[0m Trial 99 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  100\n",
            "  Number of pruned trials:  90\n",
            "  Number of complete trials:  10\n",
            "Best trial:\n",
            "  Value:  0.17754734346096207\n",
            "  Params: \n",
            "    n_layers: 3\n",
            "    n_units_l0: 53\n",
            "    dropout_l0: 0.22491663716208532\n",
            "    n_units_l1: 120\n",
            "    dropout_l1: 0.32966886065688356\n",
            "    n_units_l2: 70\n",
            "    dropout_l2: 0.40450282919630687\n",
            "    optimizer: RMSprop\n",
            "    lr: 0.002543530633186278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "Xs8pJ9CAhYSQ"
      },
      "id": "Xs8pJ9CAhYSQ"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=training_data, batch_size=BATCHSIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(dataset=validation_data, batch_size=BATCHSIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "1YsmqiJ1tg6F"
      },
      "id": "1YsmqiJ1tg6F",
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a model with the best parameter value"
      ],
      "metadata": {
        "id": "f12mES--hkqq"
      },
      "id": "f12mES--hkqq"
    },
    {
      "cell_type": "code",
      "source": [
        "class NovoNet(nn.Module):\n",
        "    def __init__(self, name='Novonet'):\n",
        "        super(NovoNet, self).__init__()\n",
        "        if name:\n",
        "            self.name = name\n",
        "        self.conv1 = nn.Conv2d(1, 2, (2,3), stride=(2, 1), padding=(1, 1))\n",
        "        self.pool = nn.MaxPool2d((2, 3), stride=(2, 2), padding=(1,0))\n",
        "        self.conv2 = nn.Conv2d(2, 6, (2,3), stride=(2, 1), padding=(1, 1))\n",
        "        self.fc1 = nn.Linear(6 * 2 * 54, 53)\n",
        "        self.fc2 = nn.Linear(53, 120)\n",
        "        self.fc3 = nn.Linear(120, 70)\n",
        "        self.fc4 = nn.Linear(70, 1)\n",
        "        self.dropout1 = nn.Dropout(p=0.22491663716208532)\n",
        "        self.dropout2 = nn.Dropout(p=0.32966886065688356)\n",
        "        self.dropout3 = nn.Dropout(p=0.40450282919630687)\n",
        "\n",
        "        # compute the total number of parameters\n",
        "        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(self.name + ': total params:', total_params)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 6 * 2 * 54)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MfxUh2yKY2zA"
      },
      "id": "MfxUh2yKY2zA",
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NovoNet(name='Novonet')\n",
        "net.to(DEVICE)"
      ],
      "metadata": {
        "id": "19skUICLfi4h",
        "outputId": "1b017a8a-18a3-4e12-d752-d593bf6812dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "19skUICLfi4h",
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Novonet: total params: 49510\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NovoNet(\n",
              "  (conv1): Conv2d(1, 2, kernel_size=(2, 3), stride=(2, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=(2, 3), stride=(2, 2), padding=(1, 0), dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(2, 6, kernel_size=(2, 3), stride=(2, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=648, out_features=53, bias=True)\n",
              "  (fc2): Linear(in_features=53, out_features=120, bias=True)\n",
              "  (fc3): Linear(in_features=120, out_features=70, bias=True)\n",
              "  (fc4): Linear(in_features=70, out_features=1, bias=True)\n",
              "  (dropout1): Dropout(p=0.22491663716208532, inplace=False)\n",
              "  (dropout2): Dropout(p=0.32966886065688356, inplace=False)\n",
              "  (dropout3): Dropout(p=0.40450282919630687, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.002543530633186278\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.RMSprop(net.parameters(), lr=LEARNING_RATE)\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "LjG-TnyJfz3t"
      },
      "id": "LjG-TnyJfz3t",
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model and store the training and validation losses and accuracy"
      ],
      "metadata": {
        "id": "jwRb6maWhzpL"
      },
      "id": "jwRb6maWhzpL"
    },
    {
      "cell_type": "code",
      "source": [
        "training_score_history = []\n",
        "training_losses_history = []\n",
        "validation_score_history = []\n",
        "validation_losses_history = []\n",
        "for epoch in range(EPOCHS):\n",
        "    net.train()\n",
        "    training_score = []\n",
        "    training_loss = []\n",
        "    for img, label in train_loader:\n",
        "      img, label = img.to(DEVICE), label.to(DEVICE)\n",
        "      label = label.unsqueeze(1)\n",
        "    \n",
        "      #==========Forward pass===============\n",
        "      preds = net(img)\n",
        "      loss = criterion(preds, label)\n",
        "      \n",
        "      #==========backward pass==============\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "      train_result = stats.spearmanr(preds.detach().cpu().numpy(), label.cpu().numpy())\n",
        "      training_score.append(train_result.correlation)\n",
        "      training_loss.append(loss.item())\n",
        "    \n",
        "    validation_score = []\n",
        "    validation_loss = []\n",
        "    for img, label in valid_loader:\n",
        "      net.eval()\n",
        "      with torch.no_grad():\n",
        "        img, label = img.to(DEVICE), label.to(DEVICE)\n",
        "        label = label.unsqueeze(1)\n",
        "      \n",
        "        val_preds = net(img)\n",
        "        val_loss = criterion(val_preds, label)\n",
        "\n",
        "        val_result = stats.spearmanr(val_preds.detach().cpu().numpy(), label.cpu().numpy())\n",
        "        validation_score.append(val_result.correlation)\n",
        "        validation_loss.append(val_loss.item())\n",
        "    training_scores = np.mean(training_score)\n",
        "    training_losses = np.mean(training_loss)\n",
        "    validation_scores = np.mean(validation_score)\n",
        "    validation_losses = np.mean(validation_loss)\n",
        "\n",
        "    training_score_history.append(training_scores)\n",
        "    training_losses_history.append(training_losses)\n",
        "    validation_score_history.append(validation_scores)\n",
        "    validation_losses_history.append(validation_losses)\n",
        "    print(f'{epoch+1:03} EPOCH - Training score : {np.mean(training_scores):.5f} | Validation score : {np.mean(validation_scores):.5f} | Training loss : {np.mean(training_losses):.5f} | Validation loss : {np.mean(validation_losses):.5f}')"
      ],
      "metadata": {
        "id": "HNVSaAcagZJl",
        "outputId": "f3178c18-e2f5-4a54-a099-7dbe6a17403c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HNVSaAcagZJl",
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "001 EPOCH - Training score : 0.14552 | Validation score : 0.20142 | Training loss : 174.84072 | Validation loss : 132.25511\n",
            "002 EPOCH - Training score : 0.15778 | Validation score : 0.21056 | Training loss : 155.05428 | Validation loss : 132.05331\n",
            "003 EPOCH - Training score : 0.17733 | Validation score : 0.21081 | Training loss : 151.20194 | Validation loss : 136.04800\n",
            "004 EPOCH - Training score : 0.17307 | Validation score : 0.21080 | Training loss : 153.28940 | Validation loss : 140.36736\n",
            "005 EPOCH - Training score : 0.16851 | Validation score : 0.19279 | Training loss : 151.77752 | Validation loss : 134.84023\n",
            "006 EPOCH - Training score : 0.17212 | Validation score : 0.18734 | Training loss : 151.74973 | Validation loss : 148.39321\n",
            "007 EPOCH - Training score : 0.15857 | Validation score : 0.20505 | Training loss : 153.44391 | Validation loss : 132.66055\n",
            "008 EPOCH - Training score : 0.15360 | Validation score : 0.20713 | Training loss : 152.21048 | Validation loss : 133.65215\n",
            "009 EPOCH - Training score : 0.16260 | Validation score : 0.19530 | Training loss : 152.04177 | Validation loss : 138.35525\n",
            "010 EPOCH - Training score : 0.17710 | Validation score : 0.19506 | Training loss : 149.96562 | Validation loss : 133.42293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the loss graph"
      ],
      "metadata": {
        "id": "KJ6tvBaQh_al"
      },
      "id": "KJ6tvBaQh_al"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Losses Training Vs Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Losses')\n",
        "plt.plot(training_losses_history, '-r')\n",
        "plt.plot(validation_losses_history, '-b')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "XM3RP3oqe4ub",
        "outputId": "db21bfaa-4f7f-4712-b7ff-45e889593c3d"
      },
      "id": "XM3RP3oqe4ub",
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7a30fd8070>]"
            ]
          },
          "metadata": {},
          "execution_count": 163
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the accuracy graph"
      ],
      "metadata": {
        "id": "Knp9mfr4iI0W"
      },
      "id": "Knp9mfr4iI0W"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Score Training Vs Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Scores')\n",
        "plt.plot(training_score_history, '-r')\n",
        "plt.plot(validation_score_history, '-b')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "Lo4SbI0ffgYM",
        "outputId": "f342d589-d3db-4cc2-ab41-20a840db8d18"
      },
      "id": "Lo4SbI0ffgYM",
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7a3107ea90>]"
            ]
          },
          "metadata": {},
          "execution_count": 164
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to predict for the test data"
      ],
      "metadata": {
        "id": "0wFq8PXlihfv"
      },
      "id": "0wFq8PXlihfv"
    },
    {
      "cell_type": "code",
      "source": [
        "def predections(dataloader):\n",
        "  test_tm_predictions = []\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for img in dataloader:\n",
        "      img = img.to(DEVICE)\n",
        "      tm_prediction = net(img)\n",
        "      test_tm_predictions.append(tm_prediction.cpu().numpy())\n",
        "  return [tm.squeeze().tolist() for tm in test_tm_predictions]"
      ],
      "metadata": {
        "id": "aw4wKC-sUZWm"
      },
      "id": "aw4wKC-sUZWm",
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm = predections(test_loader)"
      ],
      "metadata": {
        "id": "G5T-IlHVT7yO"
      },
      "id": "G5T-IlHVT7yO",
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Dataframe with seq id and tm prediction"
      ],
      "metadata": {
        "id": "mQbmdzgii8sr"
      },
      "id": "mQbmdzgii8sr"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"seq_id\":test_seq_id, \"tm\":tm })\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mPlARAuzWl3S",
        "outputId": "39bea9b2-10ed-4b65-b397-d202c676c525"
      },
      "id": "mPlARAuzWl3S",
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      seq_id         tm\n",
              "0      31390  55.642227\n",
              "1      31391  55.639450\n",
              "2      31392  56.957088\n",
              "3      31393  55.628628\n",
              "4      31394  55.639275\n",
              "...      ...        ...\n",
              "2408   33798  56.002319\n",
              "2409   33799  56.002319\n",
              "2410   33800  56.006866\n",
              "2411   33801  55.889465\n",
              "2412   33802  56.004082\n",
              "\n",
              "[2413 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f6e6910-fc15-4e21-a2b5-4420b3a05d6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seq_id</th>\n",
              "      <th>tm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31390</td>\n",
              "      <td>55.642227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31391</td>\n",
              "      <td>55.639450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31392</td>\n",
              "      <td>56.957088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31393</td>\n",
              "      <td>55.628628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31394</td>\n",
              "      <td>55.639275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2408</th>\n",
              "      <td>33798</td>\n",
              "      <td>56.002319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2409</th>\n",
              "      <td>33799</td>\n",
              "      <td>56.002319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2410</th>\n",
              "      <td>33800</td>\n",
              "      <td>56.006866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2411</th>\n",
              "      <td>33801</td>\n",
              "      <td>55.889465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2412</th>\n",
              "      <td>33802</td>\n",
              "      <td>56.004082</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2413 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f6e6910-fc15-4e21-a2b5-4420b3a05d6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f6e6910-fc15-4e21-a2b5-4420b3a05d6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f6e6910-fc15-4e21-a2b5-4420b3a05d6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()[['tm']].transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "iPY0jWPdieek",
        "outputId": "0a7257d4-0609-45b5-8907-202e6909a30f"
      },
      "id": "iPY0jWPdieek",
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     count       mean       std        min       25%        50%        75%  \\\n",
              "tm  2413.0  55.585655  0.699981  50.237091  55.57811  55.634674  55.682705   \n",
              "\n",
              "         max  \n",
              "tm  61.56295  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c58e40e2-97d5-4b49-bc63-2512d132f8c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tm</th>\n",
              "      <td>2413.0</td>\n",
              "      <td>55.585655</td>\n",
              "      <td>0.699981</td>\n",
              "      <td>50.237091</td>\n",
              "      <td>55.57811</td>\n",
              "      <td>55.634674</td>\n",
              "      <td>55.682705</td>\n",
              "      <td>61.56295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c58e40e2-97d5-4b49-bc63-2512d132f8c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c58e40e2-97d5-4b49-bc63-2512d132f8c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c58e40e2-97d5-4b49-bc63-2512d132f8c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert dataframe into csv file"
      ],
      "metadata": {
        "id": "rR3im2EtjIau"
      },
      "id": "rR3im2EtjIau"
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "fVk3q7seW-6g"
      },
      "id": "fVk3q7seW-6g",
      "execution_count": 169,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}